<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head><meta name="generator" content="Hexo 3.9.0">

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--Description-->
    
        <meta name="description" content="决策树理解划分选择


算法分类
划分选择
公式
说明



ID3（Iterative Dichotomiser）
信息增益
Gain(D,a)=Ent(D)-Σ(|D^v|/|D|)*Ent(D^v)
信息增益越大，由属性a划分而来的集合D的纯度越高，则优先将属性a作为该结点的划分依据


C4">
    

    <!--Author-->
    
        <meta name="author" content="Aubreyii">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="决策树">
    

    <!--Open Graph Description-->
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="Deep Playing">

    <!--Type page-->
    
        <meta property="og:type" content="article">
    

    <!--Page Cover-->
    

    
        <meta name="twitter:card" content="summary">
    
    
    

    <!-- Title -->
    
    <title>决策树 - Deep Playing</title>

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.2/css/bootstrap.min.css" integrity="sha384-y3tfxAZXuh4HwSYylfB+J125MxIs6mR5FOHamPBG064zB+AFeWH94NdvaCBm8qnd" crossorigin="anonymous">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/style.css">

    <!-- Google Analytics -->
    


</head>


<body>

<div class="bg-gradient"></div>
<div class="bg-pattern"></div>

<!-- Menu -->
<!--Menu Links and Overlay-->
<div class="menu-bg">
    <div class="menu-container">
        <ul>
            
            <li class="menu-item">
                <a href="/">
                    Home
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/archives">
                    Archives
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/categories">
                    Categories
                </a>
            </li>
            
        </ul>
    </div>
</div>

<!--Hamburger Icon-->
<nav>
    <a href="#menu"></a>
</nav>

<div class="container">

    <!-- Main Content -->
    <div class="row">
    <div class="col-sm-12">

        <!--Title and Logo-->
        <header>
    <div class="logo">
        <a href="/"><i class="logo-icon fa fa-cube" aria-hidden="true"></i></a>
        
    </div>
</header>

        <section class="main">
            
<div class="post">

    <div class="post-header">
        <h1 class="title">
            <a href="/2020/01/27/决策树/">
                决策树
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2020-01-27</span>
            
            
            
                <span class="category">
                    <a href="/categories/Machine-Learning/">Machine Learning</a>
                </span>
            
        </div>
    </div>

    <div class="content">

        <!-- Gallery -->
        

        <!-- Post Content -->
        <h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><h2 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h2><h3 id="划分选择"><a href="#划分选择" class="headerlink" title="划分选择"></a>划分选择</h3><table>
<thead>
<tr>
<th align="center">算法分类</th>
<th align="center">划分选择</th>
<th align="center">公式</th>
<th align="center">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center">ID3（Iterative Dichotomiser）</td>
<td align="center">信息增益</td>
<td align="center">Gain(D,a)=Ent(D)-Σ(|D^v|/|D|)*Ent(D^v)</td>
<td align="center">信息增益越大，由属性a划分而来的集合D的纯度越高，则优先将属性a作为该结点的划分依据</td>
</tr>
<tr>
<td align="center">C4.5</td>
<td align="center">增益率</td>
<td align="center">Gain_ratio(D,a)=Gain(D,a)/IV(a)</td>
<td align="center">信息增益除以属性a的固有值，找出增益率最高的，属性a可取数目越多，固有值越大。需要注意的是，增益率准则对可取数目较少的属性有所偏好。</td>
</tr>
<tr>
<td align="center">CART</td>
<td align="center">基尼指数</td>
<td align="center">Gini(D)=1-Σpk^2</td>
<td align="center">Gini(D)反映了从数据集D种随机挑两个样本，类别不一致的概率。所以基尼值越小，纯度越高。</td>
</tr>
</tbody></table>
<h3 id="剪枝处理"><a href="#剪枝处理" class="headerlink" title="剪枝处理"></a>剪枝处理</h3><h4 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a>预剪枝</h4><p>基于信息增益准则，选择属性对训练集进行划分，然后再通过验证集进行检验前后划分验证集精度（其实就是预测的正确率），如果提升则进行该次划分。</p>
<p><strong>优点：</strong>预剪枝使得决策树的很多分支都没有展开，降低过拟合的风险。</p>
<p><strong>缺点：</strong>有些分支的当前划分虽不能提升泛化性能、甚至可能导致泛化性能暂时下降，但在其基础上进行的后续划分却有可能导致性能显著提高，所以预剪枝决策树有欠拟合的风险。</p>
<h4 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h4><p>先生成一颗决策树，从最后的节点开始剪枝，如果有利于提高准确度就剪掉。</p>
<p>优点：保留了更多的分支，后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树</p>
<p>缺点：后剪枝过程是在生成完全决策树之后进行的，并且自底向上地对树中所有非叶结点进行逐一考察，因此其训练时间开销比未剪枝决策树和预剪枝决策树都要大得多。</p>
<h3 id="连续与缺失值"><a href="#连续与缺失值" class="headerlink" title="连续与缺失值"></a>连续与缺失值</h3><h4 id="连续值处理"><a href="#连续值处理" class="headerlink" title="连续值处理"></a>连续值处理</h4><p>确立划分点，划分点是能够使该属性信息增益最大的点。比如一个属性为密度，划分点为0.381，则可以&lt;0.381或者&gt;0.381作为种类的划分。<strong>需要注意的是，与离散属性不同，若当前结点划分属性为连续属性，该属性还可作为其后代结点的划分属性（即可以划分区间如&lt;0.381的子结点仍然可以划分出&gt;0.294）</strong></p>
<h4 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h4><p>若某样本属性a缺失，则在进行通过属性a划分时，将该样本同时划分到各个子结点中，只不过要赋予权重，权重为  各个子结点划分得到的样本数/该结点待划分的样本数</p>
<h3 id="多变量决策树"><a href="#多变量决策树" class="headerlink" title="多变量决策树"></a>多变量决策树</h3><p>在一个结点里有多个属性参与划分，在多变量决策树的学习过程中，不是为每个非叶结点寻找一个最优属性，而是试图建立一个合适的线性分类器。比如：</p>
<p>​                                                      -8.00×密度 - 0.044×含糖率&lt;=-0.313</p>
<p>​                                                        ↓是                                         ↓否</p>
<p>​                                                    好瓜                                         坏瓜</p>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><h3 id="源码实现"><a href="#源码实现" class="headerlink" title="源码实现"></a>源码实现</h3><h3 id="调库"><a href="#调库" class="headerlink" title="调库"></a>调库</h3><h4 id="sklearn中DecisionTreeClassifier重要参数"><a href="#sklearn中DecisionTreeClassifier重要参数" class="headerlink" title="sklearn中DecisionTreeClassifier重要参数"></a>sklearn中DecisionTreeClassifier重要参数</h4><p>sklearn中决策树算法参数共有13个，如下：</p>
<p>class sklearn.tree.DecisionTreeClassifier(criterion=’gini’, splitter=’best’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weigh=None, presort=False)</p>
<p><strong>A.重要参数：criterion</strong></p>
<p>划分选择</p>
<p>(1) “entropy”，使用信息熵</p>
<p>(2) “gini”,使用基尼系数（默认）</p>
<blockquote>
<p>信息熵对不纯度更加敏感，对不纯度的惩罚最强。</p>
</blockquote>
<p><strong>B.重要参数：random_state&amp;splitte</strong></p>
<p>random_state用来设置分枝中的随机模式的参数，在高维度时随机性会表现更明显。</p>
<p>输入任意整数，会一直长出同一颗树，让模型稳定下来</p>
<p>splitter也是用来控制决策树中的随机选项的，有两种输入值，输入”best”，决策树在分枝时虽然随机，但是还是会优先选择更重要的特征进行分枝（重要性可以通过属性feature_importances_查看），输入“random”，决策树在分枝时会更加随机，树会因为含有更多的不必要信息而更深更大，并因这些不必要信息而降低对训练集的拟合。这也是防止过拟合的一种方式。当你预测到你的模型会过拟合，用这两个参数来帮助你降低树建成之后过拟合的可能性。当然，树一旦建成，我们依然是使用剪枝参数来<strong>防止过拟合</strong>。</p>
<p><strong>C.重要参数：剪枝参数</strong></p>
<p><strong>（1）max_depth</strong></p>
<p>限制树的最大深度，超过设定深度的树枝全部剪掉</p>
<p><strong>（2）min_samples_leaf</strong></p>
<p>min_samples_leaf（最少的节点）  每个分支下至少有该参数数量的节点，否则分枝就不会产生，或者朝着min_samples_leaf方向去发生。</p>
<p>一般搭配max_depth使用，在回归树中可以让模型变得更加平滑。太小引起过拟合。</p>
<blockquote>
<p>一般来说建议5开始使用。</p>
</blockquote>
<p><strong>(3)min_samples_split</strong></p>
<p>min_samples_split限定，一个节点必须要包含至少min_samples_split个训练样本，这个节点才允许被分枝，否则分枝就不会发生。</p>
<p><strong>(4)min_features</strong></p>
<p> max_features限制分枝时考虑的特征个数，超过限制个数的特征都会被舍弃</p>
<p><strong>(5)min_impurity_decrease</strong></p>
<p>min_impurity_decrease限制信息增益的大小，信息增益小于设定数值的分枝不会发生。</p>
<p><strong>D.确认最优的剪枝参数</strong></p>
<p>其实就是运用循环改变参数，确定出准确度最高的参数（也就是最优参数）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_wine</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"> </span><br><span class="line">wine = load_wine()</span><br><span class="line"> </span><br><span class="line">xtrain,xtest,ytrain,ytest = train_test_split(wine.data,wine.target,test_size=<span class="number">0.3</span>)</span><br><span class="line"> </span><br><span class="line">clf=tree.DecisionTreeClassifier(criterion=<span class="string">"entropy"</span></span><br><span class="line">,random_state=<span class="number">30</span></span><br><span class="line">                                ,splitter=<span class="string">"random"</span></span><br><span class="line">,max_depth=<span class="number">3</span></span><br><span class="line">                                ,min_samples_leaf=<span class="number">10</span></span><br><span class="line">                                ,min_samples_split=<span class="number">70</span></span><br><span class="line">                                )</span><br><span class="line">clf=clf.fit(xtrain,ytrain)  <span class="comment">#把训练数据集放入到分类器中，fit来寻找相应标签</span></span><br><span class="line">score = clf.score(xtest,ytest) <span class="comment">#score是针对上面模型评估精确度</span></span><br><span class="line">print(score)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> graphviz</span><br><span class="line">feature_name = [<span class="string">'酒精'</span>,<span class="string">'苹果酸'</span>,<span class="string">'灰'</span>,<span class="string">'灰的碱性'</span>,<span class="string">'镁'</span>,<span class="string">'总酚'</span></span><br><span class="line">,<span class="string">'类黄酮'</span>,<span class="string">'非黄烷类酚类'</span>,<span class="string">'花青素'</span>,<span class="string">'颜 色强度'</span>,<span class="string">'色调'</span>,<span class="string">'od280/od315稀释葡萄酒'</span>,<span class="string">'脯氨酸'</span>]</span><br><span class="line">dot_data = tree.export_graphviz(clf</span><br><span class="line">                                , feature_names=feature_name</span><br><span class="line">                                , class_names=[<span class="string">"琴酒"</span>, <span class="string">"雪莉"</span>, <span class="string">"贝尔摩德"</span>]</span><br><span class="line">                                , filled=<span class="literal">True</span></span><br><span class="line">, rounded=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"> </span><br><span class="line">graph = graphviz.Source(dot_data)</span><br><span class="line">graph.view()</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">test = []f</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    clf=tree.DecisionTreeClassifier(max_depth=i+<span class="number">1</span></span><br><span class="line">                                    ,criterion=<span class="string">"entropy"</span></span><br><span class="line">,random_state=<span class="number">30</span></span><br><span class="line">                                    )</span><br><span class="line">    clf=clf.fit(xtrain,ytrain)</span><br><span class="line">    score_train = clf.score(xtrain, ytrain)</span><br><span class="line">    test.append(score)</span><br><span class="line">plt.plot(range(<span class="number">1</span>,<span class="number">11</span>),test,color=<span class="string">'red'</span>,label=<span class="string">'max_depth'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><strong>E.目标权重参数</strong></p>
<p>(1)class_weight</p>
<p>完成样本标签平衡的参数。比如判断“信用卡持有人是否违约”的比例为（1％：99%）。这种分类状态，即便模型什么也不做，全部结果预测“否”，正确率也能有99%。因此我们要使用class_weight参数对样本标签进行一定的均衡，给少量的标签更多的权重，让模型更偏向少数类，向捕获少数类的方向建模。该参数默认None，此模式表示自动给与数据集中的所有标签相同的权重。</p>
<p>(2)min_weight_fraction_leaf</p>
<p>有了权重之后，样本量就不再是单纯地记录数目，而是受输入的权重影响了，因此这时候剪枝，就需要搭配min_ weight_fraction_leaf这个基于权重的剪枝参数来使用。另请注意，基于权重的剪枝参数（例如min_weight_ fraction_leaf）将比不知道样本权重的标准（比如min_samples_leaf）更少偏向主导类。如果样本是加权的，则使用基于权重的预修剪标准来更容易优化树结构，这确保叶节点至少包含样本权重的总和的一小部分。</p>
<p><strong>F.重要属性和接口</strong></p>
<p>最重要的属性是feature_importances_，能够查看各个特征对模型的重要性</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">clf=tree.DecisionTreeClassifier()</span><br><span class="line">feature_importances=clf.feature_importances_</span><br><span class="line"># 输出feature就会得到各个特征对模型的重要性</span><br><span class="line">clf.fit(xtrain,ytrain)</span><br><span class="line">clf.score(xtest,ytest)</span><br><span class="line">clf.apply(xtest)  #返回</span><br><span class="line">clf.predict(xtest) #输出预测结果</span><br></pre></td></tr></table></figure>

<p>fit(训练模型)    score(准确性)   apply(输入测试集返回每个测试样本所在的叶子节点的索引)</p>
<p>predict(预测)</p>

    </div>

    

    

    <!-- Comments -->
    

</div>
        </section>

    </div>
</div>


</div>

<!-- Footer -->
<div class="push"></div>

<footer class="footer-content">
    <div class="container">
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-6 col-lg-6 footer-about">
                <h2>About</h2>
                <p>
                    
                </p>
            </div>
            
    <div class="col-xs-6 col-sm-6 col-md-3 col-lg-3 recent-posts">
        <h2>Recent Posts</h2>
        <ul>
            
            <li>
                <a class="footer-post" href="/2020/03/12/为终端配置代理/">为终端配置代理</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2020/03/02/LightGBM-gridsearchcv/">LightGBM+gridsearchcv</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2020/02/04/数据可视化/">数据可视化</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2020/01/27/决策树/">决策树</a>
            </li>
            
        </ul>
    </div>



            
<div class="col-xs-6 col-sm-6 col-md-3 col-lg-3 footer-categories">
    <h2>Categories</h2>
    <ul>
        
        <li>
            <a class="footer-post" href="/categories/散装知识/">散装知识</a>
        </li>
        
        <li>
            <a class="footer-post" href="/categories/前端/">前端</a>
        </li>
        
        <li>
            <a class="footer-post" href="/categories/Machine-Learning/">Machine Learning</a>
        </li>
        
        <li>
            <a class="footer-post" href="/categories/javaweb/">javaweb</a>
        </li>
        
    </ul>
</div>

        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <ul class="list-inline footer-social-icons">
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                </ul>
            </div>
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <div class="footer-copyright">
                    
                </div>
            </div>
        </div>
    </div>
</footer>

<!-- After footer scripts -->

<!-- jQuery -->
<script src="//code.jquery.com/jquery-2.1.4.min.js"></script>

<!-- Tween Max -->
<script src="//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.5/TweenMax.min.js"></script>

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Custom JavaScript -->
<script src="/js/main.js"></script>

<!-- Disqus Comments -->



</body>

</html>